{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignement2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNFXidZQs4BZp+gk0tnTux7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahroo12/COMP-551-Assignment-2/blob/mahroo/Assignement2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlDrf6U0vCsd"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiMBv9yKoE4V"
      },
      "source": [
        "class MultiClassLogisticRegression:\n",
        "    def __init__(self, add_bias=True, learning_rate=.1, epsilon=1e-4, max_iters=1e5):\n",
        "        self.add_bias = add_bias\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epsilon = epsilon                        \n",
        "        self.max_iters = max_iters   \n",
        "\n",
        "    def softmax(self, x):\n",
        "        exps = np.exp(x - np.max(x))\n",
        "        norms = nd.sum(exps)\n",
        "        return exps / norms  \n",
        "\n",
        "    def one_hot_encoding(self, y):\n",
        "        #y_u = pd.get_dummies(self.y).values\n",
        "        #return y_u \n",
        "        u_y = list(np.unique(y))\n",
        "        encoded = np.zeros((len(y)), (len(u_y)))\n",
        "        for i, c in enumerate(y):\n",
        "           encoded[i][u_y.index(c)] = 1\n",
        "        return encoded\n",
        "           \n",
        "    def fit(self, x, y, optimizer):\n",
        "        if x.ndim == 1:\n",
        "            x = x[:, None]\n",
        "        if self.add_bias:\n",
        "            N = x.shape[0]\n",
        "            x = np.column_stack([x,np.ones(N)])\n",
        "        N,D = x.shape\n",
        "        \n",
        "        def gradient(x, y, w):                          # more like an error\n",
        "            y_h = one_hot_encoding(self.y)\n",
        "            yh = softmax(np.dot(x,self.w))\n",
        "            #yh = softmax(x @ w)\n",
        "            N, D = x.shape\n",
        "            grad = .5*np.dot(yh - y_h, x)/N\n",
        "            return grad\n",
        "        w0 = np.zeros(D)                                # initialize the weights to 0\n",
        "        self.w = optimizer.run(gradient, x, y, w0)      # run the optimizer to get the optimal weights\n",
        "        return self\n",
        "    \n",
        "    def predict(self, x):\n",
        "        if x.ndim == 1:\n",
        "            x = x[:, None]\n",
        "        Nt = x.shape[0]\n",
        "        if self.add_bias:\n",
        "            x = np.column_stack([x,np.ones(Nt)])\n",
        "        yh = softmax(np.dot(x,self.w))            #predict output\n",
        "        return yh"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seysXbDOubig"
      },
      "source": [
        "Get the matrix for x\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_PC5xO4ug-4"
      },
      "source": [
        "\"\"\"\n",
        "N,D = x.shape # where N takes shape[0] or the number of instances or number of rows\n",
        "              #where D takes shape[1] or the number of features or number of columns\n",
        "# Step 1: initialize weight to 0\n",
        "# where the makes 0 number of columns with 1 row. \n",
        "#usually matrices of weight have each row represent a class and each column represent a feature\n",
        "self.w = np.zeros(D)\n",
        "# Step 2: need to find hot encoding\n",
        "def one_hot_encoding(self, y):\n",
        "  u_y = list(np.unique(y))\n",
        "  encoded = np.zeros((len(y)), (len(u_y)))\n",
        "  for i, c in enumerate(y):\n",
        "    encoded[i][u_y.index(c)] = 1\n",
        "  return encoded\n",
        "\n",
        "#Step 3: softmax\n",
        "\n",
        "#step 3.1: find softmax formula\n",
        "\n",
        "def softmax(self, x):\n",
        "    exps = np.exp(x - np.max(x))\n",
        "    norms = nd.sum(exps)\n",
        "    return exps / norms\n",
        "\n",
        "#step 3.1: apply softmax formula to wtx\n",
        "def probabilities(self, x):\n",
        "      if x.ndim == 1:\n",
        "         x = x[:, None]\n",
        "      Nt = x.shape[0]\n",
        "      if self.add_bias:\n",
        "        x = np.column_stack([x,np.ones(Nt)])\n",
        "      yh = softmax(np.dot(x,self.w))            \n",
        "      return yh\n",
        "#step 4: find cost or entropy\n",
        "\n",
        "#step 4.2: find cross entropy formula\n",
        "def crossEntropy(yhat, y):\n",
        "    return -nd.sum(y*nd.log(yhat+1e-6))\n",
        "# now have to loop over to find the whole thing\n",
        "\n",
        "for in in range(n_iter):\n",
        "  cost \n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}