{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mahroo12/COMP-551-Assignment-2/blob/main/Comp551_Project2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTUPugYI0uJz"
   },
   "source": [
    "**Import necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6jsO1TzMWzs5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-48f7cc9a5350>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpandas\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mmath\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mget_ipython\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_line_magic\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'matplotlib'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'inline'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mmatplotlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpyplot\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse\n",
    "from IPython.core.debugger import set_trace\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4J6_p3mV6Y4"
   },
   "source": [
    "**Part 1: Read in and Process Data**\n",
    "\n",
    "\n",
    "1.   Import Digits Dataset\n",
    "[digits_datatest](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html)\n",
    "2.   Import Iris Dataset\n",
    "[iris_dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "9QgIStoEVxy_",
    "outputId": "cce391b2-a0d1-4518-d616-22f509d7c815"
   },
   "outputs": [],
   "source": [
    "# Import digits dataset\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "# Additional OpenML dataset used in this project is the Iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hh9kYlwJWCAF"
   },
   "source": [
    "**Part 2: Soft Max Regression**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qpNlGnsi6fwS"
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Gradient Descent w/ Momentum Optimizer #\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPmm9TnbpjVy"
   },
   "outputs": [],
   "source": [
    "class MiniBatchGradientDescent:\n",
    "\n",
    "  def __init__(self,learning_rate=.001,max_iters=1e4,epsilon=1e-8, record_history=False,batch_size=32):\n",
    "    self.learning_rate = learning_rate      # alpha value\n",
    "    self.max_iters = max_iters              # epchos = passes through data set\n",
    "    self.record_history = record_history\n",
    "    self.epsilon = epsilon                  # termination condition\n",
    "    self.batch_size = batch_size            # if 1, then SGD\n",
    "\n",
    "    if record_history:\n",
    "      self.w_history = []\n",
    "\n",
    "  #################################################\n",
    "  # METHOD: Get Batch from Data Set\n",
    "  #################################################\n",
    "  # PARAMETERS:\n",
    "  # X: training vectors, shape = [n_samples,n_features] = NxD\n",
    "  # y: target values (classes), shape = [n_samples] = N\n",
    "  # i: batch to get\n",
    "  # RETURNS:\n",
    "  # X: data for this batch\n",
    "  # y: classes for this batch\n",
    "  #################################################\n",
    "  def get_batches(self,X,y,i):\n",
    "      X_new = X[i:i+self.batch_size,:] #,: added to end of this for some reason?\n",
    "      y_new = y[i:i+self.batch_size]\n",
    "      return X_new, y_new\n",
    "\n",
    "\n",
    "  #################################################\n",
    "  # METHOD: Mini-Batch Gradient Descent\n",
    "  #################################################\n",
    "  # PARAMETERS:\n",
    "  # X: training vectors, shape = [n_samples,n_features]\n",
    "  # y: target values, shape = [n_samples]\n",
    "  # w: initial weight used in calculation\n",
    "  # RETURNS:\n",
    "  # m: final weight (used for optimization)\n",
    "  #################################################\n",
    "  def run(self,gradient_fn,X,y,w):\n",
    "    num_batches = int(X.shape[0]/self.batch_size)    # number of batches = samples / batch size\n",
    "    gradient = np.inf                                # initialize gradient to infinity\n",
    "    beta = 0.1\n",
    "    grad = np.inf\n",
    "    delta_w = 0\n",
    "\n",
    "    i = 1\n",
    "    while np.linalg.norm(grad) > self.epsilon and i < self.max_iters:\n",
    "\n",
    "      for t in range(0,num_batches):\n",
    "        X_batch, y_batch = self.get_batches(X,y,t)\n",
    "\n",
    "        grad = gradient_fn(X_batch,y_batch,w)\n",
    "\n",
    "        delta_w = beta * delta_w + (1-beta) * grad\n",
    "        w = w - self.learning_rate * delta_w.T\n",
    "\n",
    "        if self.record_history:\n",
    "          self.w_history.append(w)\n",
    "\n",
    "      i += 1\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPXupe0tOdob"
   },
   "source": [
    "**Multi-class Logistic Regression**\n",
    "\n",
    "$\\sigma(z) = \\frac{1}{1+e^{-z}}$ is the logistic function, called sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyAOM2qQumHF"
   },
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression:\n",
    "\n",
    "  # initialize model parameters\n",
    "  def __init__(self, add_bias=True, learning_rate=.1, epsilon=1e-4, max_iters=1e5):\n",
    "    self.add_bias = add_bias\n",
    "    self.learning_rate = learning_rate\n",
    "    self.epsilon = epsilon\n",
    "    self.max_iters = max_iters\n",
    "    self.w = []\n",
    "\n",
    "  # softmax function takes an N-dimensional vector X of real numbers\n",
    "  # transforms into valid probability distribution\n",
    "  def softmax(self, X):\n",
    "    exps = np.exp(X - np.max(X))      # exponentiates, forces positive values\n",
    "    norms = np.sum(exps)              # normalizes values to sum to 1\n",
    "    return exps / norms\n",
    "\n",
    "\n",
    "  # encode categorical variable values to numbers\n",
    "  # add 1s for unique classes and 0s in rest of matrix\n",
    "  def one_hot_encoding(self, y):\n",
    "    m = y.shape[0]\n",
    "    OHX = scipy.sparse.csr_matrix((np.ones(m), (y, np.array(range(m)))))\n",
    "    OHX = np.array(OHX.todense()).T\n",
    "    return OHX\n",
    "\n",
    "  # maximizes log likelihood given correct labels\n",
    "  def crossEntropy(yhat,y):\n",
    "    return - nd.sum(y * nd.log(yhat+1e-6))\n",
    "\n",
    "  #################################################\n",
    "  # METHOD: learn model from training data\n",
    "  #################################################\n",
    "  # PARAMETERS:\n",
    "  # X: training vectors, shape = [n_samples,n_features]\n",
    "  # y: target values, shape = [n_samples]\n",
    "  # optimizer: function used for optimization\n",
    "  # RETURNS:\n",
    "  # self: object\n",
    "  #################################################\n",
    "  def fit(self, X, y, optimizer):\n",
    "    if X.ndim == 1:\n",
    "      X = X[:, None]\n",
    "    if self.add_bias:\n",
    "      N = X.shape[0]\n",
    "      X = np.column_stack([X,np.ones(N)])           # add vector of 1s\n",
    "    N,D = X.shape                                   # N samples, D features\n",
    "\n",
    "    def gradient(X, y, w):\n",
    "      N,D = X.shape\n",
    "      yh = self.softmax(np.dot(X,w.T))      # softmax on scores to get probabilities\n",
    "      #print('yh soft: ', yh.shape)\n",
    "      y_mat = self.one_hot_encoding(y)    # convert classs to one-hot representation\n",
    "      #print('shape y_mat: ', y_mat.shape)\n",
    "      #loss = (-1/N) * np.sum(y_mat * np.log(yh))\n",
    "      try:\n",
    "        grad = (-1/N) * np.dot(X.T,(yh-y))\n",
    "        #grad = (-1/N) * np.dot(X.T,(yh-y_mat))\n",
    "        #grad = 0.5*np.dot(yh-y_true, X)/N\n",
    "        return grad\n",
    "\n",
    "      except ValueError as e:\n",
    "        print(e)\n",
    "        print('shape N: ', N)\n",
    "        print('shape D: ', D)\n",
    "        print('shape yh: ', yh.shape)\n",
    "        print('shape y_mat: ', y_mat.shape)\n",
    "\n",
    "    w0 = np.zeros([len(np.unique(y)),D])            # initialize the weights to [classes, features]\n",
    "    self.w = optimizer.run(gradient, X, y, w0)      # run the optimizer to get the optimal weights\n",
    "\n",
    "    return self\n",
    "\n",
    "  #####################################################\n",
    "  # METHOD: Predict targets from X\n",
    "  #####################################################\n",
    "  # PARAMETERS:\n",
    "  # X: training vectors, shape = [n_samples,n_features]\n",
    "  # RETURNS:\n",
    "  # yh: predicted target values, shape = [n_samples]\n",
    "  #####################################################\n",
    "  def predict(self, X):\n",
    "    if X.ndim == 1:\n",
    "      X = X[:, None]\n",
    "    N = X.shape[0]\n",
    "    if self.add_bias:\n",
    "      X = np.column_stack([X,np.ones(N)])         # add bias vetor\n",
    "    yh = self.softmax(np.dot(X,self.w.T))           # predict output yh = softmax(xW + b)\n",
    "    return yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oyorh3r9Irmg"
   },
   "outputs": [],
   "source": [
    "# loaded training data digits data set\n",
    "m_digits = digits.data.shape[0]  # number of samples\n",
    "X_digits = digits.data           # shape=[1797,64]=[n_samples,n_features]\n",
    "y_digits = digits.target_names   # names of target classes  (0,1...9)\n",
    "\n",
    "# define an optimizer\n",
    "optimizer = MiniBatchGradientDescent(learning_rate=.005, max_iters=100, record_history=True, batch_size=m_digits)\n",
    "\n",
    "# define a model\n",
    "model = MultiClassLogisticRegression()\n",
    "\n",
    "# pass an instance of the optimizer to the model, fit() method will run this to fit the data\n",
    "model.fit(X_digits,y_digits,optimizer)\n",
    "\n",
    "model.predict(X_digits)\n",
    "#print('w history:', optimizer.w_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cF4iociWvEzB"
   },
   "source": [
    "**Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6vVJ2mBOWODT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8fZFNKwWGid"
   },
   "source": [
    "**Part 3: Testing of Soft Max**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uC4M5c2TWQ0r"
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvOxYqLUWJia"
   },
   "source": [
    "**Part 4: Alternative Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6eVYGzoZWRvB"
   },
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Comp551_Project2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}