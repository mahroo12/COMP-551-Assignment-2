{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mahroo12/COMP-551-Assignment-2/blob/main/Comp551_Project2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTUPugYI0uJz"
   },
   "source": [
    "**Import necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6jsO1TzMWzs5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.debugger import set_trace\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4J6_p3mV6Y4"
   },
   "source": [
    "**Part 1: Read in and Process Data**\n",
    "\n",
    "\n",
    "1.   Import Digits Dataset\n",
    "[digits_datatest](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html)\n",
    "2.   Import Iris Dataset\n",
    "[iris_dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "9QgIStoEVxy_",
    "outputId": "cce391b2-a0d1-4518-d616-22f509d7c815"
   },
   "outputs": [],
   "source": [
    "# Import digits dataset\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "print(digits.data.shape) #1797 samples, 64 features each\n",
    "plt.imshow(digits.images[0]) #show digit \n",
    "\n",
    "# Additional OpenML dataset used in this project is the Iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "print(iris.data.shape) #150 samples, 4 features each\n",
    "iris.target[[10, 25, 50]] #example showing data\n",
    "list(iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hh9kYlwJWCAF"
   },
   "source": [
    "**Part 2: Soft Max Regression**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qpNlGnsi6fwS"
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Gradient Descent w/ Momentum Optimizer #\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPmm9TnbpjVy"
   },
   "outputs": [],
   "source": [
    "class MiniBatchGradientDescent:\n",
    "\n",
    "  def __init__(self,learning_rate=.001,max_iters=1e4,epsilon=1e-8, record_history=False,batch_size=32):\n",
    "    self.learning_rate = learning_rate\n",
    "    self.max_iters = max_iters\n",
    "    self.record_history = record_history\n",
    "    self.epsilon = epsilon #termination condition\n",
    "    self.batch_size = batch_size\n",
    "    \n",
    "    if record_history:\n",
    "      self.w_history = []                 #to store the weight history for visualization\n",
    "\n",
    "  # Function for dividing the data into batches\n",
    "  # X is data, NxD\n",
    "  # y is classes, N\n",
    "  def get_batches(X,y,i):\n",
    "      X_new = X[i:i+self.batch_size,:] #,: added to end of this for some reason?\n",
    "      y_new = y[i:i+self.batch_size]    \n",
    "      return X_new, y_new\n",
    "\n",
    "  def gradient_fn(X,y,m,b):\n",
    "    n_points = X.shape[0]\n",
    "    m_grad = 0\n",
    "    b_grad = 0\n",
    "\n",
    "    for i in range(n_points):\n",
    "      #get current pair (x,y)\n",
    "      xi = X[i,0]\n",
    "      yi = y[i,1]\n",
    "\n",
    "      #prevent crashing if missing data\n",
    "      if (math.isnan(xi)|math.isnan(yi)):\n",
    "        continue\n",
    "\n",
    "      #partial derivatives\n",
    "      dm = -((2/n_points)*xi*(yi-(m*xi+b)))\n",
    "      db = -((2/n_points)*(yi-(m*xi+b)))\n",
    "\n",
    "      #update gradient\n",
    "      m_grad = m_grad+dm\n",
    "      b_grad=b_grad+db\n",
    "\n",
    "    #update working m and b\n",
    "    m_update = m - self.learning_rate*m_grad\n",
    "    b_update = b - self.learning_rate*b_grad\n",
    "\n",
    "    return m_update,b_update \n",
    "\n",
    "  # Mini-Batch Gradient Descent\n",
    "  def run(self,gradient_fn,X,y,w):\n",
    "      num_batches = int(X.shape[0]/self.batch_size)    \n",
    "      gradient = np.inf\n",
    "      i = 1 #iteration counter \n",
    "      m=0\n",
    "      b=w\n",
    "      while i < self.max_iters: #for each iteration\n",
    "        for t in range(0,num_batches): #for each mini batch\n",
    "            X_batch, y_batch = get_batches(X,y,t) #get X and y for this mini batch\n",
    "            m,b = gradient_fn(X_batch, y_batch,m,b)\n",
    "        i += 1 # next iteration\n",
    "      print('m ',m)\n",
    "      print('b ',b)\n",
    "      return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPXupe0tOdob"
   },
   "source": [
    "**Multi-class Logistic Regression**\n",
    "\n",
    "$\\sigma(z) = \\frac{1}{1+e^{-z}}$ is the logistic function, called sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyAOM2qQumHF"
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "# Soft Max Regression Model #\n",
    "##############################\n",
    "class SoftMaxRegression:\n",
    "  # initialize model and add bias / intercept\n",
    "  def __init__(self, add_bias=True):\n",
    "    self.add_bias = add_bias\n",
    "    pass\n",
    "\n",
    "  # softmax function takes an N-dimensional vector X of real numbers\n",
    "  # transforms into probability distribution\n",
    "  # normalize values in vector to make function more stable at extreme values\n",
    "  # makes values non-negative and sum to 1 \n",
    "  def softmax(X):\n",
    "    exps = np.exp(X - np.max(X))\n",
    "    norms = nd.sum(exps)\n",
    "    return exps / norms\n",
    "\n",
    "  # cross entropy loss\n",
    "  def crossEntropy(yhat, y):\n",
    "    return -nd.sum(y*nd.log(yhat+1e-6))\n",
    "\n",
    "  # for continuous classificaiton values\n",
    "  def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oyorh3r9Irmg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cF4iociWvEzB"
   },
   "source": [
    "**Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6vVJ2mBOWODT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8fZFNKwWGid"
   },
   "source": [
    "**Part 3: Testing of Soft Max**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uC4M5c2TWQ0r"
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvOxYqLUWJia"
   },
   "source": [
    "**Part 4: Alternative Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6eVYGzoZWRvB"
   },
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Comp551_Project2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
